{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d6193-c1e4-4c40-af35-4853e6f9784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import threading\n",
    "\n",
    "# Crear la sesión de Spark con los JARs necesarios\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumerWithLimit\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir el esquema para el JSON\n",
    "schema = StructType([\n",
    "    StructField(\"uuid\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"workplace_uuid\", StringType(), True),\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"light\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),\n",
    "    StructField(\"co2\", DoubleType(), True),\n",
    "    StructField(\"s0\", DoubleType(), True),\n",
    "    StructField(\"no2\", DoubleType(), True),\n",
    "    StructField(\"o3\", DoubleType(), True),\n",
    "    StructField(\"position_x\", DoubleType(), True),\n",
    "    StructField(\"position_y\", DoubleType(), True),\n",
    "    StructField(\"position_z\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d382fe22-25a5-4fe4-ae10-75b18a2e7d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'version 2.12.18'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._jvm.scala.util.Properties.versionString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba242e30-00b4-443e-9c2d-b2fea3e156cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable global para contar el número total de registros procesados\n",
    "total_records_processed = 0\n",
    "max_records = 150000\n",
    "\n",
    "# Leer el stream de Kafka\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"invernadero\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\").option(\"maxOffsetsPerTrigger\", \"1000\").load()\n",
    "\n",
    "\n",
    "# Convertir el valor de los mensajes de Kafka de binario a cadena\n",
    "df_string = df_raw.selectExpr(\"CAST(value AS STRING) as json_string\")\n",
    "\n",
    "# Parsear el JSON y obtener un DataFrame estructurado\n",
    "df_parsed = df_string.select(from_json(col(\"json_string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Convertir el campo timestamp a tipo Timestamp y agregar columna de fecha\n",
    "df_parsed = df_parsed \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"fecha\", to_date(\"timestamp\"))\n",
    "\n",
    "# Función para procesar cada micro-batch\n",
    "def process_batch(df, epoch_id):\n",
    "    global total_records_processed, max_records, query\n",
    "\n",
    "    batch_count = df.count()\n",
    "    total_records_processed += batch_count\n",
    "\n",
    "    # Procesar el batch (por ejemplo, escribir en almacenamiento)\n",
    "    df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"workplace_uuid\", \"fecha\") \\\n",
    "        .save(\"/home/jovyan/work/data\")\n",
    "\n",
    "    print(f\"Batch {epoch_id}: Procesados {batch_count} registros, total acumulado: {total_records_processed}\")\n",
    "\n",
    "    if total_records_processed >= max_records:\n",
    "        print(\"Se ha alcanzado el límite de registros. Deteniendo la consulta...\")\n",
    "        # Detener la consulta de streaming\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf525556-b155-4db6-9db4-b0604b3870fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10154: Procesados 6 registros, total acumulado: 6\n",
      "Batch 10155: Procesados 1000 registros, total acumulado: 1006\n",
      "Batch 10156: Procesados 1000 registros, total acumulado: 2006\n",
      "Batch 10157: Procesados 1000 registros, total acumulado: 3006\n",
      "Batch 10158: Procesados 1000 registros, total acumulado: 4006\n",
      "Batch 10159: Procesados 1000 registros, total acumulado: 5006\n",
      "Batch 10160: Procesados 1000 registros, total acumulado: 6006\n",
      "Batch 10161: Procesados 1000 registros, total acumulado: 7006\n",
      "Batch 10162: Procesados 1000 registros, total acumulado: 8006\n",
      "Batch 10163: Procesados 1000 registros, total acumulado: 9006\n",
      "Batch 10164: Procesados 1000 registros, total acumulado: 10006\n",
      "Batch 10165: Procesados 1000 registros, total acumulado: 11006\n",
      "Batch 10166: Procesados 1000 registros, total acumulado: 12006\n",
      "Batch 10167: Procesados 1000 registros, total acumulado: 13006\n",
      "Batch 10168: Procesados 1000 registros, total acumulado: 14006\n",
      "Batch 10169: Procesados 1000 registros, total acumulado: 15006\n",
      "Batch 10170: Procesados 1000 registros, total acumulado: 16006\n",
      "Batch 10171: Procesados 1000 registros, total acumulado: 17006\n",
      "Batch 10172: Procesados 1000 registros, total acumulado: 18006\n",
      "Batch 10173: Procesados 1000 registros, total acumulado: 19006\n",
      "Batch 10174: Procesados 1000 registros, total acumulado: 20006\n",
      "Batch 10175: Procesados 1000 registros, total acumulado: 21006\n",
      "Batch 10176: Procesados 1000 registros, total acumulado: 22006\n",
      "Batch 10177: Procesados 1000 registros, total acumulado: 23006\n",
      "Batch 10178: Procesados 1000 registros, total acumulado: 24006\n",
      "Batch 10179: Procesados 1000 registros, total acumulado: 25006\n",
      "Batch 10180: Procesados 1000 registros, total acumulado: 26006\n",
      "Batch 10181: Procesados 1000 registros, total acumulado: 27006\n",
      "Batch 10182: Procesados 1000 registros, total acumulado: 28006\n",
      "Batch 10183: Procesados 1000 registros, total acumulado: 29006\n",
      "Batch 10184: Procesados 1000 registros, total acumulado: 30006\n",
      "Batch 10185: Procesados 1000 registros, total acumulado: 31006\n",
      "Batch 10186: Procesados 1000 registros, total acumulado: 32006\n",
      "Batch 10187: Procesados 1000 registros, total acumulado: 33006\n",
      "Batch 10188: Procesados 1000 registros, total acumulado: 34006\n",
      "Batch 10189: Procesados 1000 registros, total acumulado: 35006\n",
      "Batch 10190: Procesados 1000 registros, total acumulado: 36006\n",
      "Batch 10191: Procesados 1000 registros, total acumulado: 37006\n",
      "Batch 10192: Procesados 1000 registros, total acumulado: 38006\n",
      "Batch 10193: Procesados 1000 registros, total acumulado: 39006\n",
      "Batch 10194: Procesados 1000 registros, total acumulado: 40006\n",
      "Batch 10195: Procesados 1000 registros, total acumulado: 41006\n",
      "Batch 10196: Procesados 1000 registros, total acumulado: 42006\n",
      "Batch 10197: Procesados 1000 registros, total acumulado: 43006\n",
      "Batch 10198: Procesados 1000 registros, total acumulado: 44006\n",
      "Batch 10199: Procesados 1000 registros, total acumulado: 45006\n",
      "Batch 10200: Procesados 1000 registros, total acumulado: 46006\n",
      "Batch 10201: Procesados 1000 registros, total acumulado: 47006\n",
      "Batch 10202: Procesados 1000 registros, total acumulado: 48006\n",
      "Batch 10203: Procesados 1000 registros, total acumulado: 49006\n",
      "Batch 10204: Procesados 1000 registros, total acumulado: 50006\n",
      "Se ha alcanzado el límite de registros. Deteniendo la consulta...\n"
     ]
    }
   ],
   "source": [
    "# Iniciar la consulta de streaming con foreachBatch\n",
    "query = df_parsed.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/work/checkpoints\") \\\n",
    "    .start()\n",
    "\n",
    "# Mantener el stream en ejecución\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f8fac-d4de-40e1-8733-484d4bae9416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
